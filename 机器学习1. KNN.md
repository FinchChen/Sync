---
title: 机器学习1. KNN
date: 2019-11-21
tags: 
    - kNN
    - k-近邻算法
categories: 机器学习
---

# 机器学习实战笔记1 K-近邻算法

## 1. 概述

KNN算法采用测量不同特征值之间的距离方法进行分类. 一般来说,只选择样本数据集中前k个最相似的数据,来确定新数据的分类.

* 优点: 精度高,对异常值不敏感,无数据输入假定
* 缺点: 计算复杂度高,空间复杂度高


## 2. KNN分类算法实现

### 2.1 生成数据集
``` python
import numpy as np
import operator

def creatDataSet():
    group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
    labels = ['A','A','B','B']
    return group, labels
```

### 2.2 分析图像
``` python
import matplotlib.pyplot as plt

dataset, labels = creatDataSet()
plt.scatter(dataset[:,0],dataset[:,1])
```

### 2.3 kNN分类伪代码
    
    对未知类别属性的数据集中的每一个点依次执行:
    1. 计算已知类别数据集中的点与当前点之间的距离
    2. 按照距离递增次序排序
    3. 选取与当前点距离最小的k个点
    4. 确定前k个点所在类别的出现频率
    5. 返回前k个点出现频率最高的类别当作当前点的预测分类

``` python
def classify0(inX, dataset, labels, k):
    datasetSize = dataset.shape[0]
    diffMat = np.tile(inX, (datasetSize,1)) - dataset
    # np.tile(x,shape) 在shape中构造重复个x
    # np.tile([1,2],(3,1)) = [1,2];[1,2];[1,2]
    # diffMat就是距离
    sqDiffMat = diffMat ** 2
    sqDistance = sqDiffMat.sum(axis = 1) # 行距离相加
    distances = sqDistance ** 0.5
    sortedDistIndices = distances.argsort() # 返回datasetSize个index,[0,1,3,2],越小越近
    classCount={}
    for i in range(k):
        voteIlabel = labels[sortedDistIndices[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) +1 # classCount {'A':2,'B':1}
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    # sortedClassCount [('A', 2), ('B', 1)]
    return sortedClassCount [0][0]
```


## 3. 在约会网站上使用kNN算法

### 3.1 准备数据

训练集可在[ 这里 ](https://www.manning.com/downloads/1108)获取

原版书用的是python2.7和numpy,我更倾向于pandas的dataframe,所以就不重复造轮子了.

``` python
import pandas as pd

PATH = '/home/finch/data/'
test = pd.read_table(f'{PATH}Ch02/datingTestSet2.txt',header = None)
datingDataMat = test[[0,1,2]].copy()
datingLabels = test[[3]].copy()
```

### 3.2 散点图

这里分别用datingDataMat里的第2,3列和第1,2列画图

``` python
import matplotlib.pyplot as plt

plt.xlabel('Video Game Percentage (%)')
plt.ylabel('Weekly Icecream Eaten (L)')

plt.scatter(datingDataMat[1],datingDataMat[2],
           s = 10*np.array(datingLabels[datingLabels.columns[0]]),
           c = np.array(datingLabels[datingLabels.columns[0]]))
# s 大小, c 颜色

plt.xlabel('Yearly Flying Points (miles)')
plt.ylabel('Video Game Percentage (%)')
plt.scatter(datingDataMat[0],datingDataMat[1],
           s = 8*np.array(datingLabels[datingLabels.columns[0]]),
           c = np.array(datingLabels[datingLabels.columns[0]]))
```
得出来的两幅图就跟书上一样了

### 3.3 特征值归一化

与其使用真实值,应该把~~大的~~**所有的**数值normalize一下.

``` python
datingDataMat = (datingDataMat-datingDataMat.min(0))/datingDataMat.max(0)
```

### 3.4 测试(训练)

由于2.2里我使用的是原书上的numpy结构,datingClassTest也需要从dataframe转化成np.array

``` python
def datingClassTest():
    hoRatio = 0.1
    test = pd.read_table(f'{PATH}Ch02/datingTestSet2.txt',header = None)
    datingDataMat = test[[0,1,2]].copy()
    datingLabels = test[[3]].copy()
    normMat = (datingDataMat-datingDataMat.min(0))/datingDataMat.max(0)
    m = normMat.shape[0]
    numTestVecs = int(m*hoRatio)
    errorCount = 0.0
    for i in range(numTestVecs): #随机选10%
        classifierResult = classify0(normMat.iloc[i].to_numpy(),normMat.iloc[numTestVecs:m].to_numpy(),datingLabels.iloc[:,0][numTestVecs:m].values,5)
        print ("the classifier came back with: %d, the real answer is: %d" % (classifierResult,int(datingLabels.iloc[i])))
        if classifierResult != int(datingLabels.iloc[i]):
            errorCount += 1
    print ("the total error rate is: %f" % (errorCount/float(numTestVecs)))
```

得到的准确率接近于94%

## 4. 使用kNN构建手写识别系统

偷个懒...我就直接复制源码了
``` python
import os

def img2vector(filename):
    returnVect = np.zeros((1,1024))
    fr = open(filename)
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect[0,32*i+j] = int(lineStr[j])
    return returnVect

def handwritingClassTest():
    hwLabels = []
    trainingFileList = os.listdir(f'{PATH}Ch02/trainingDigits')          #load the training set
    m = len(trainingFileList)
    trainingMat = np.zeros((m,1024))
    for i in range(m):
        fileNameStr = trainingFileList[i]
        fileStr = fileNameStr.split('.')[0]     #take off .txt
        classNumStr = int(fileStr.split('_')[0])
        hwLabels.append(classNumStr)
        trainingMat[i,:] = img2vector(f'{PATH}Ch02/trainingDigits/%s' % fileNameStr)
    testFileList = os.listdir(f'{PATH}Ch02/testDigits')        #iterate through the test set
    errorCount = 0.0
    mTest = len(testFileList)
    for i in range(mTest):
        fileNameStr = testFileList[i]
        fileStr = fileNameStr.split('.')[0]     #take off .txt
        classNumStr = int(fileStr.split('_')[0])
        vectorUnderTest = img2vector(f'{PATH}Ch02/testDigits/%s' % fileNameStr)
        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
        print ("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr))
        if (classifierResult != classNumStr): errorCount += 1.0
    print ("\nthe total number of errors is: %d" % errorCount)
    print ("\nthe total error rate is: %f" % (errorCount/float(mTest)))
```
核心思想是:
+ 32x32的txt格式图像
+ 每个图像转化为1x1024的向量
+ 然后统计trainingset有m个图片
+ 于是数据集 = m x 1024的矩阵
+ 用的是for循环一个一个做的比较...每个做1024*m次浮点运算...所以效率非常低

## 5. 小结

kNN是最简单的分类算法.其缺点是无法给出任何数据的基础结构信息,因此也无从知晓平均实例样本和典型实例样本具有什么特征~~这两个玩意是啥~~.
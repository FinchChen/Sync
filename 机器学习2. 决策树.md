---
title: 机器学习2. 决策树
date: 2019-11-21 19:11:00
tags: 
    - 决策树
    - Decision Tree
categories: 机器学习

---

# 机器学习实战笔记2 决策树

## 1. 概述

优点: 计算复杂度不高,输出结果易于理解,对中间值的缺失不敏感,可以处理不相关特征数据
缺点: 容易过拟合

## 2. 构造决策树

### 2.1 信息增益(information gain)

构造决策树时,第一个要解决的问题是,当前数据集上哪个特征在划分数据分类时起决定作用. 所以要评估每个特征.(递归)

一些决策树使用二分法,本书使用ID3算法,不生成二叉树.

在划分数据集前后信息发生的变化称为信息增益.计算每个特征分类后的信息增益,最高的则是最好的划分特征.

计算信息增益的方式为香农熵.**熵的定义为 信息的期望值**

(数学公式在hexo里面好麻烦..以后有空再补)

香农熵代码实现:
``` python
from math import log

def calcShannonEnt(dataset):
    numEntries = len(dataset)
    labelCounts = {}
    for featVec in dataset:
        currentLabel = featVec[-1] #最后一个是标签?
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts: #遍历
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob,2) #信息的定义
    return shannonEnt
```
简单来说,熵越高,混合的数据类型也越多.

``` python
def creatDataSet():
    dataset = [[1,1,'yes'],
              [1,1,'yes'],
              [1,0,'no'],
              [0,1,'no'],
              [0,1,'no']]
    labels = ['no surfacing','flippers']
    return dataset, labels

dataset, labels = creatDataSet()
calcShannonEnt(dataset)
# 0.9709505944546686
dataset[0][-1] = 'maybe'
calcShannonEnt(dataset)
# 1.3709505944546687
dataset[0][-1] = 'no'
dataset[1][-1] = 'no'
calcShannonEnt(dataset)
# 0.0
```

得到熵之后,就可以按照获取最大信息增益的方法划分数据集.另一个度量集合无序程度的方法是*基尼不纯度*.书里说略过...

### 2.2 划分数据集

按照给定特征划分数据集:
``` python
def splitDataSet(dataset, axis, value):
    retDataset = []
    for featVec in dataset:#遍历
        if featVec[axis] == value: 
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:]) #去掉featVec[axis]这个值
            retDataset.append(reducedFeatVec)
    return retDataset #返回符合条件的数据集
```
遍历,选择最好的数据集划分方式:
```python
def chooseBestFeatureToSplit(dataset):
    numFeatures = len(dataset[0]) - 1
    baseEntropy = calcShannonEnt(dataset)
    bestInfoGain = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataset]
        uniqueVals = set(featList) #取得唯一值 set里的不重复
        newEntropy = 0.0
        for value in uniqueVals:
            subDataset = splitDataSet(dataset,i,value)
            prob = len(subDataset)/float(len(dataset))
            newEntropy += prob * calcShannonEnt(subDataset)
        infoGain = baseEntropy - newEntropy #infogain越大,熵越小 信息增益=熵的减小
        if infoGain > bestInfoGain:
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
```

### 2.3 递归构建决策树

这一节字好多...自己看书吧

当数据集处理完了所有属性,但是子节点依旧含有不同标签,这时就用多数表决的方法决定该子节点的标签

``` python
import operator

def majorityCnt(classList):
    classCount = {}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.items(),
                             key = operatoe.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
```
后续还会介绍C4.5,CART等决策树算法,这里先实现ID3算法来构造树:
```python
def createTree(dataset, labels):
    classList = [example[-1] for example in dataset]
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 类别完全相同则停止划分
    if len(dataset[0]) == 1:
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataset)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}} # 嵌套字典
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataset]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset,bestFeat,value),subLabels) # 递归
    return myTree
```
```python
dataset,labels = creatDataSet()
myTree = createTree(dataset, labels)
myTree
# {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
```

***
Top-down:

1. 选择最好的特征 (chooseBestFeatureToSplit)
3. 递归子树 (createTree)
3. 如果子树全是同一类别就停
4. 所有属性全处理完,就把大部分当作类别 (majorityCnt)


## 3. 用matplotlib注解绘制树形图

决策树的主要优点就是直观,易于理解,如果不能直观的将其显示出来,就无法发挥其优势

### 3.1 Matplotlib注解

我觉得这个画出来的好丑...就直接贴代码不贴过程了

```python
import matplotlib.pyplot as plt

decisionNode = dict(boxstyle="sawtooth", fc = "0.8")
leafNode = dict(boxstyle="round4", fc= "0.8")
arrow_args = dict(arrowstyle="<-")

def plotNode(nodeText, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeText, xy=parentPt,
                           xycoords='axes fraction',xytext = centerPt, textcoords = 'axes fraction',
                           va="center", ha = "center", bbox=nodeType, arrowprops = arrow_args)
def createPlot():
    fig = plt.figure(1,facecolor='white')
    fig.clf()
    createPlot.ax1 = plt.subplot(111, frameon=False)
    plotNode('decision node', (0.5,0.1), (0.1,0.5), decisionNode)
    plotNode('leaf node', (0.8,0.1), (0.3,0.8), leafNode)
    plt.show()

createPlot()
```
![decisiontree1.png](https://i.loli.net/2019/11/23/zBgeUNSrP14tuwC.png)

再添加三个函数,获取叶节点的数目和树的层数,输出预先存储的树的信息
``` python
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            numLeafs += getNumLeafs(secondDict[key])
        else:   numLeafs +=1
    return numLeafs

def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:   thisDepth = 1
        if thisDepth > maxDepth: maxDepth = thisDepth
    return maxDepth

def retrieveTree(i):
    listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},
                  {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
                  ]
    return listOfTrees[i]
```
直接上完整版了:**记得把之前的createPlot注释掉**
``` python
def plotMidText(cntrPt, parentPt, txtString):
    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]
    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)

def plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on
    numLeafs = getNumLeafs(myTree)  #this determines the x width of this tree
    depth = getTreeDepth(myTree)
    firstStr = list(myTree.keys())[0]     #the text label for this node should be this
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)
    plotMidText(cntrPt, parentPt, nodeTxt)
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes   
            plotTree(secondDict[key],cntrPt,str(key))        #recursion
        else:   #it's a leaf node print the leaf node
            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD
#if you do get a dictonary you know it's a tree, and the first element will be another dict

def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks
    #createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses 
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;
    plotTree(inTree, (0.5,1.0), '')
    plt.show()

```
``` python
myTree = retrieveTree(1)
createPlot(myTree)
```
![decisiontree2.png](https://i.loli.net/2019/11/23/T2Ix917nLok6jCv.png)

## 4. 测试分类器与存储

### 4.1 测试算法

``` python
def classify(inputTree,featLabels,testVec):
    firstStr = inputTree.keys()[0]
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    key = testVec[featIndex]
    valueOfFeat = secondDict[key]
    if isinstance(valueOfFeat, dict): 
        classLabel = classify(valueOfFeat, featLabels, testVec)
    else: classLabel = valueOfFeat
    return classLabel
# classify(mytree, labels, [1,0])
```
缺点: 不能分类[1,3],因为[1,3]不在树里

### 4.2 存储与调用

``` python
def storeTree(inputTree,filename):
    import pickle
    fw = open(filename,'w')
    pickle.dump(inputTree,fw)
    fw.close()
    
def grabTree(filename):
    import pickle
    fr = open(filename)
    return pickle.load(fr)
```

## 5. 实例 用决策树预测隐形眼镜类型

``` python
PATH = '/home/finch/data/Ch03/'
fr = open(f'{PATH}lenses.txt')
lenses = [inst.strip().split('\t') for inst in fr.readlines()]
lensesLables = ['age', 'prescript', 'astigmatic', 'testRate']
lensesTree = createTree(lenses, lensesLables)
createPlot(lensesTree)
```
![decisiontree3.png](https://i.loli.net/2019/11/23/4uyblHjOM6odLkh.png)

**第9章将进一步讨论过拟合与CART算法的问题**

# 文本分类算法迭代流程

## v1.0 Hanlp

### v1.1 Hanlp文本聚类

书中示例有83%的F1 score,但在实际应用在搜狗新闻-体育语料中,F1只有70%,详见 *Hanlp项目1.md*

k-means聚类的缺点: 聚类的个数难以确定,自动分类能分出100多个小类别(1000篇文章),无法确定是不是我们想要的分类

由于python版本主要调用的是java的库,对其并不熟悉,故换用别的模型

### v1.2 文本分类

详见 *Hanlp项目1.md*

跑通了demo, 但是是Java的IDE里面跑的,我没有选择修改模型,而是在v2中把它的步骤用python来实现

## v2.0 tfidf + SVM + jieba + flask

### v2.1 语料: 搜狗新闻语料库

这里还是使用搜狗新闻的十分类语料库来做基础模型搭建

### v2.1.1 文本,预处理,清洗,规范化

与其使用原本的:
* 文件夹1
    * 文件1.txt
    * 文件2.txt
    * ....
* 文件夹2

机器学习任务中更多的采用的是单csv文件来调用数据

```
classification | content
__label__Game | 伊苏新作登陆PSV首支预告片公布FALCOM正式发表PSVita平...
__label__Affairs | 以色列总理内塔尼亚胡上任首次出访埃及中新网5月6日电法新社报道...
__label__Furnishing | 2011中国家居空气质量大调查世界卫生组织公布世界卫生报告指出室内 ...
__label__Affairs | 德国女首富遭骗色骗财舍弃形象诱捕疑犯图一个专骗女人钱瑞士小白...
```
具体步骤实现:
1. 将所有的文本内容合并并加上相应的标签
2. 去除重复的文章
3. 去除停用词,标点符号
4. 打乱顺序
5. 保存为一个csv文件

### v2.2 分词: jieba分词库

Hanlp使用起来没有很方便,所以采用python可直接调用的jieba分词库

应用在刚刚的示例上面的效果:

```
classification | content
__label__Game | 伊苏 新作 登陆 PSV 首支 预告片 公布 FALCOM 正式 发表 PS Vita 平...
__label__Affairs | 以色列 总理 内塔尼亚胡 上任 首次 出访 埃及 中新网 5 月 6 日电 法新社 报道 ...
__label__Furnishing | 2011 中国 家居 空气质量 大 调查 世界卫生组织 公布 世界卫生 报告 指出 室内 ...
__label__Affairs | 德国 女 首富 遭骗色 骗财 舍弃 形象 诱捕 疑犯 图 一个 专骗 女人 钱 瑞士 小白...
```


### v2.3 文章向量化: tfidf算法

python中可以调用sklearn库:

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
```
其中CountVectorizer是词袋one-hot向量模型,tfidf做的是词频统计以及加权,这样避免了同一文章中的重复出现的词汇扰乱了频率的权重.

1. 所以首先我将**所有的**数据从csv中导入,以此训练一个完整的加权词频向量
```python
vectorizer = CountVectorizer(min_df=1e-5) # drop df < 1e-5,去低频词
transformer = TfidfTransformer()
sentences_tfidf = transformer.fit_transform(vectorizer.fit_transform(sentences))

import pickle
feature_path = f'{PATH}model/feature_17class_almost_done.pkl'
with open(feature_path, 'wb') as fw:
    pickle.dump(vectorizer.vocabulary_, fw)
tfidftransformer_path = f'{PATH}model/tfidftransformer_17class_almost_done.pkl'
with open(tfidftransformer_path, 'wb') as fw:
    pickle.dump(transformer, fw)
```
将得到的词向量模型,tfidf模型分别保存,供预测服务端调用

```python
words = vectorizer.get_feature_names()

print ("how many words: {0}".format(len(words)))
print ("tf-idf shape: ({0},{1})".format(sentences_tfidf.shape[0], sentences_tfidf.shape[1]))
```
```
how many words: 502287
tf-idf shape: (66734,502287)
```
可以看出总共有66734篇文章, 一篇文章的tfidf向量长度为502287 (可以理解为502287个词)

2. 然后在这一步在划分训练集与验证,测试集:
```python
X_train, X_test, y_train, y_test = train_test_split(
    sentences_tfidf, y, test_size=0.2, random_state=1000)
```
### v2.4 分类模型选用:

有三个可以候选:
```python
from sklearn.svm import LinearSVC # SVM 支持向量机
from sklearn.naive_bayes import MultinomialNB # NB 朴素贝叶斯
from sklearn.linear_model import LogisticRegression # LR 逻辑回归
```
经过测试, LinearSVC实现可以同时满足最大精确度与最快速度.

注: LinearSVC是使用了线性kernel的SVM,对比其他核函数的SVM,分类速度有大幅提升

```python
classifier = LinearSVC()
%time classifier.fit(X_train, y_train)
score = classifier.score(X_test, y_test)

print("Accuracy:", score)
```
搜狗十分类新闻语料库得到的成绩是93%.

### v2.5 换语料: 八文库现有分好类的所有文章

拿到明哥那边的所有语料之后,同样对其进行了预处理步骤,不重复说明

因为其中的文章不是正文而是包含了html标签,于是使用正则删除中文以外的字符

得到的结果:

```
              precision    recall  f1-score   support

          体育       0.79      0.82      0.80       195
        健康养生       0.75      0.84      0.79       181
          其他       0.64      0.83      0.72        95
          军事       0.83      0.87      0.85       201
          动漫       0.88      0.86      0.87       196
          国际       0.86      0.78      0.82       211
          娱乐       0.61      0.64      0.62       203
          家居       0.76      0.68      0.72       237
          情感       0.88      0.70      0.78       213
          教育       0.80      0.75      0.78       186
          文化       0.66      0.72      0.69       190
          旅游       0.76      0.83      0.79       169
          时事       0.87      0.83      0.85       191
          时尚       0.76      0.68      0.72       228
        星座运势       0.78      0.91      0.84       175
        母婴育儿       0.83      0.87      0.85       202
          汽车       0.71      0.70      0.70       210
          游戏       0.84      0.76      0.80       248
          社会       0.73      0.80      0.76       198
          科技       0.75      0.63      0.69       249
          综合       0.54      0.66      0.60       186
          美食       0.71      0.78      0.74       174
          财经       0.69      0.62      0.65       205
          音乐       0.82      0.83      0.83       185

    accuracy                           0.76      4728
   macro avg       0.76      0.77      0.76      4728
weighted avg       0.76      0.76      0.76      4728
```

远不如搜狗新闻语料的0.93优秀

尝试使用过 *python的正文提取库*, *罗工的正文提取api*,效果并没有很大的提升

后来判断为八文库本身文章分类有缺陷,导致模型无法拟合

**得出解决方案: 自定义一级分类与二级分类,并根据子标签去爬取对应的多篇文章构造语料库**


## v3.0 BERT

这一步实际上是跟v2.tfidf同时进行,为模型分类效果不好的一种尝试

### v3.1 BERT框架 + fastai

这里进入**深度学习**的领域.前面使用的都是**传统机器学习**的方法.

注: 难点在于:
1. BERT模型非常的复杂,短时间内很难搞懂并从源码上做出改变
2. 现有的教学文章都是基于**英文**来实现的,需要针对**中文**对模型做出适配
3. 同上,BERT的优点在于预训练模型,使其他下游任务(比如分类)能达到更高的精确度,**中文**又是一个难点
4. 多分类任务与二分类任务不同,也需要修改模型

查阅**大量**文档,发现了一个可以跑中文的魔改版的demo并部署在了google colab(因为GPU没到).

预处理完搜狗十分类语料后,使用BERT训练可达**0.94**的准确率,相应的代价是1.5G的模型与2小时的训练时间

这一步感觉也还不错,使用八文库语料训练:

> accuracy  = 0.03

排查后发现是语料质量不高导致神经网络震荡无法收敛

### v3.2 Flask

确定是语料的问题后,决定先搭建能用的服务,再去优化模型精确度.

在Flask和tornado,Django中选择了**Flask**(最简单,轻量化)

搭建出的 tfidf+SVM 模型 QPS 为: 30
BERT 模型 QPS 为: 10

引出深度思考:

工业级的服务部署中,是否需要深度学习?深度学习能带来的优点只有不多的精确度和召回率(5%),代价却是很长的训练时间,预测时间,以及模型大小(占用内存),是否可以牺牲一部分准确率来保证高并发场景下的部署?

## v4.0 Fasttext + jieba + japronto

### v4.1 Fasttext

再次广泛的搜索文档,Facebook的Fasttext在工业界应用中出现的频率比较高,而且也适配中文.

``` python
PATH = 'G:/临时/'
import fasttext
import numpy as np
import os

def train_model(ipt=None, opt=None, model='', dim=100, epoch=5, lr=0.1, loss='softmax'):
    np.set_printoptions(suppress=True)
    if os.path.isfile(model):
        classifier = fasttext.load_model(model)
    else:
        classifier = fasttext.train_supervised(ipt, label='__label__', dim=dim, epoch=epoch,
                                         lr=lr, wordNgrams=2, loss=loss)
        """
          训练一个监督模型, 返回一个模型对象
          
          @param input:           训练数据文件路径
          @param lr:              学习率
          @param dim:             向量维度
          @param ws:              cbow模型时使用
          @param epoch:           次数
          @param minCount:        词频阈值, 小于该值在初始化时会过滤掉
          @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉
          @param minn:            构造subword时最小char个数
          @param maxn:            构造subword时最大char个数
          @param neg:             负采样
          @param wordNgrams:      n-gram个数
          @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax
          @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量
          @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出
          @param lrUpdateRate:    学习率更新
          @param t:               负采样阈值
          @param label:           类别前缀
          @param verbose:         ??
          @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机
          @return model object
        """
        classifier.save_model(opt)
    return classifier

dim = 100
epoch = 5
lr=1
model = f'data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}_v1.model'

classifier = train_model(ipt='55555_train.txt',
                         opt=model,
                         model=model,
                         dim=dim, epoch=epoch, lr=1
                         )

result = classifier.test('55555_test.txt')
print(result)
```

Fasttext原理是训练出文章中每个词的词向量,并构造浅层神经网络,实现端对端的文本分类.

1. 英文不需要,中文则需要分词,以及去除停用词的操作,并且在预测的输入中保持相同的操作
2. 词向量包含了n-gram模型,可以结合n个词组成复合词,提升上下文联系
3. 训练时间不长,大概10分钟
4. 模型大小可以控制(100维词向量大概是1G)

### v4.2 语料选择

* 因为对爬虫去爬取文章还是有点不自信(和抗拒),于是我又找了很多现成的语料,其中发现问答类的语料分类的很准确,我就把问答类语料里面答句长度在**1000字**以上的挑选出来了22个分类组成语料库.
* 22个分类放入Fasttext里面,效果非常好,F1达到了**0.90**,证明了语料才是模型的上限
* 剔除掉几个F1只有0.5的分类后,F1达到了**0.961**,类别还剩下**17**个,分别是:
> 时政,教育,娱乐,财经,装修,游戏,股票,生活,体育,科技,汽车,情感,美食,健康,宠物,旅游,军事

* 确定以该模型为可上线模型.

注: 解释一下上一节所用的 55555_train.txt 和 55555_text.txt 的构造过程:

1. 同样的第一步先将拿到的语料进行预处理, jieba分词, 去停用词, 打乱, 保存为 55555.csv

```
label   content
__label__Shares	赴 港 上市 再 升温 金风 科技 拟发 2 47 亿 H股 本报记者 孙洁琳 前有 中国...
__label__Sports	郭跃 李晓霞 演绎 一生 之敌 国乒 未来 手中 新浪 体育讯 北京 时间 5 月 4 日...
__label__war	中国 式 主战 坦克 日本 式 主战 坦克 优秀 谢谢 中国 对决 日本 中国 式 主战 ...
__label__war	古印度 国王 武器 摩诃 婆罗 公元前 或前 世纪 罗摩衍那 约 公元前 或前 世纪 公元...
__label__Furnishing	揭秘 全球 10 大 最具 个性 魅力 豪宅 组图 10 特别 豪宅 杜塞尔多夫 汽车 别...
```

每个分类前面要加__label__(这是Fasttext规定的)

2. 划分测试集, 验证集
``` python
import pandas as pd
from random import shuffle

def split_train_test(source, auth_data=False):
    if not auth_data:
        train_proportion = 0.8
    else:
        train_proportion = 0.98

    basename = source.rsplit('.', 1)[0]
    train_file = basename + '_train.txt'
    test_file = basename + '_test.txt'

    handel = pd.read_csv(source, index_col=False, low_memory=False)
    train_data_set = []
    test_data_set = []
    for head in list(handel.head())[1:]:
        train_num = int(handel[head].dropna().__len__() * train_proportion)
        sub_list = [f'__label__{head} , {item.strip()}\n' for item in handel[head].dropna().tolist()]
        train_data_set.extend(sub_list[:train_num])
        test_data_set.extend(sub_list[train_num:])
    shuffle(train_data_set)
    shuffle(test_data_set)

    with open(train_file, 'w', encoding='utf-8') as trainf,\
        open(test_file, 'w', encoding='utf-8') as testf:
        for tds in train_data_set:
            trainf.write(tds)
        for i in test_data_set:
            testf.write(i)

    return train_file, test_file

train_file, test_file = split_train_test('55555.csv')
```

得到 55555_train.txt 和 55555_text.txt

### V4.3 Japronto与效果对比

罗工找到了一个超级厉害的框架Japronto,尝试搭建后与Flask做对比.

Japronto + tfidf + SVM : 150(QPS) (瓶颈在于tfidf模型和SVM模型的CPU占用)
Japronto + Fasttext : 4000(QPS)

Japronto服务端部署详见 *文本分类服务器端搭建流程.md*

附Japronto服务端代码:
```python
from japronto import Application

import fasttext
from preprocess import chinese_raw_text_preprocess

mapper2 = {
    '__label__Affairs':'时政',
    '__label__Education':'教育',
    '__label__Entertainment':'娱乐',
    '__label__Finance':'财经',
    '__label__Furnishing':'装修',
    '__label__Game':'游戏',
    '__label__Shares':'股票',
    '__label__Sociology':'生活',
    '__label__Sports':'体育',
    '__label__Technology':'科技',
    '__label__car':'汽车',
    '__label__motion':'情感',
    '__label__food':'美食',
    '__label__health':'健康',
    '__label__pet':'宠物',
    '__label__tour':'旅游',
    '__label__war':'军事'
}

model_path = 'data_dim100_lr01_iter5.model'
classifier = fasttext.load_model(model_path)

def fasttext(request, methods = ["POST"]):
    if request.method == "POST":

        sentence_to_predict = chinese_raw_text_preprocess(request.form['content'])
        #print(sentence_to_predict)
        result = classifier.predict(sentence_to_predict)

        dict1 = dict()
        dict1['lv1_tag'] = {'score':result[1][0],
                   'tag':mapper2[result[0][0]]}
        #print (dict1)    
        return request.Response(json=dict1)
    return request.Response(text='please use POST')

app = Application()
app.router.add_route('/', fasttext)
app.run(port=3334,debug=True)
```

注: Japronto之所以这么快是因为它自动使用了协程, 效果跟我自己写的异步协程版本QPS相差无几

接口文档可以查看: *分类api文档v1.txt*

### v4.4 接下来要做的步骤

1. 跟法明合作, 测试接口是否达到上线的标准, 并与现有的百度云分类做对比, 找出改进的点
2. 给智图文库的所有文章都进行分类, 邓哥建议哪怕模型没有特别好, 也可以尝试先做这个步骤, 以后好修改
3. 学习推荐系统, 准备着手内容推荐引擎


总结与想法:
1. 可以在Fasttext模型中加入别人预训练好的词向量 (完成)
    * 实践过后,预训练好的中文词向量模型有300维,训练好后的模型占用内存4G,效果从0.961提升至0.97,故放弃.

2. **把完整的一,二级分类树做出来,并且每个二级分类有5篇以上不同的代表性文章**,以此为出发点再去爬取没有的内容加入进55555.csv
3. 配置一下nginx, 开多个服务做负载均衡
4. 给自己写的程序加注释, 整理, 把测试的那些去掉



*陈亦凡*
*2020.2.12*





